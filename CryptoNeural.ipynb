{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymongo\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "# all imports for step 5\n",
    "import torch\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# !pip install torchsample\n",
    "# from torchsample.modules import ModuleTrainer\n",
    "\n",
    "\n",
    "\n",
    "from sklearn import preprocessing\n",
    "import pdb\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gzip\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "# !pip install torchsample\n",
    "# from torchsample.modules import ModuleTrainer\n",
    "\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import (\n",
    "    pack_padded_sequence, pad_packed_sequence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrderbookDataset(Dataset):\n",
    "    def __init__(self,percentage):\n",
    "        self.percentage = percentage\n",
    "        \n",
    "        client = pymongo.MongoClient('localhost',27017)\n",
    "        db = client['shrimpy_binance_btc_usd']\n",
    "        self.collection = db['entries']\n",
    "        all_length = self.collection.find().count()\n",
    "        self.percentage = (all_length - 32000)/all_length\n",
    "        \n",
    "        self.real_length = int(all_length*percentage) + 16\n",
    "        self.first_time = self.collection.find_one()\n",
    "        self.first_time = self.first_time['_id'] \n",
    "        self.extra_time = 0\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        try:\n",
    "            ret = self.collection.find_one({'_id':self.first_time+60*item+self.extra_time})\n",
    "            data = ret['data']\n",
    "            label = ret['label']\n",
    "            data = np.array(data)\n",
    "            if label == 1:\n",
    "                label = np.array(0)\n",
    "            elif label == -1:\n",
    "                label = np.array(2)\n",
    "            else:\n",
    "                label = np.array(1)\n",
    "            return data,label\n",
    "        except:\n",
    "            self.extra_time += 60\n",
    "            self.get_item(item)\n",
    "        \n",
    "    def get_item(self, item):\n",
    "        try:\n",
    "            ret = self.collection.find_one({'_id':self.first_time+60*item+self.extra_time})\n",
    "            data = ret['data']\n",
    "            label = ret['label']\n",
    "            data = np.array(data)\n",
    "            if label == 1:\n",
    "                label = np.array(0)\n",
    "            elif label == -1:\n",
    "                label = np.array(2)\n",
    "            else:\n",
    "                label = np.array(1)\n",
    "            return data,label\n",
    "        except:\n",
    "            self.extra_time += 60\n",
    "            self.get_item(item)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.real_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrderbookTestDataset(Dataset):\n",
    "    def __init__(self,percentage):\n",
    "        self.percentage = percentage\n",
    "        client = pymongo.MongoClient('localhost',27017)\n",
    "        db = client['shrimpy_binance_btc_usd']\n",
    "        self.collection = db['entries']\n",
    "        all_length = self.collection.find().count()\n",
    "        self.real_length = 32000\n",
    "        self.real_length = int(self.real_length) if self.real_length == int(self.real_length) else int(self.real_length) + 1\n",
    "        self.first_time = self.collection.find_one()\n",
    "        self.first_time = self.first_time['_id']\n",
    "        self.first_time = self.collection.find_one({'_id':self.first_time+(all_length-self.real_length)*60})\n",
    "        self.first_time = self.first_time['_id']\n",
    "        \n",
    "    def __getitem__(self, item):\n",
    "        ret = self.collection.find_one({'_id':self.first_time+60*item})\n",
    "        data = ret['data']\n",
    "        label = ret['label']\n",
    "        data = np.array(data)\n",
    "        if label == 1:\n",
    "            label = np.array(0)\n",
    "        elif label == -1:\n",
    "            label = np.array(2)\n",
    "        else:\n",
    "            label = np.array(1)\n",
    "        return data,label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.real_length\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikopi/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:8: DeprecationWarning: count is deprecated. Use Collection.count_documents instead.\n",
      "  \n",
      "/home/nikopi/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:7: DeprecationWarning: count is deprecated. Use Collection.count_documents instead.\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "train = OrderbookDataset(0.9)\n",
    "test = OrderbookTestDataset(0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PadPackedSequence(nn.Module):\n",
    "    \"\"\"Some Information about PadPackedSequence\"\"\"\n",
    "    def __init__(self, batch_first=True):\n",
    "        super(PadPackedSequence, self).__init__()\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        max_length = lengths.max().item()\n",
    "        x, _ = pad_packed_sequence(\n",
    "            x, batch_first=self.batch_first, total_length=max_length)\n",
    "        return x\n",
    "\n",
    "\n",
    "class PackSequence(nn.Module):\n",
    "    def __init__(self, batch_first=True):\n",
    "        super(PackSequence, self).__init__()\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        x = pack_padded_sequence(\n",
    "            x, lengths,\n",
    "            batch_first=self.batch_first,\n",
    "            enforce_sorted=False)\n",
    "        lengths = lengths[x.sorted_indices]\n",
    "        return x, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, batch_first=True,\n",
    "                 layers=1, bidirectional=False, merge_bi='cat', dropout=0,\n",
    "                 rnn_type='lstm', packed_sequence=True, device='cpu'):\n",
    "\n",
    "        super(RNN, self).__init__()\n",
    "        self.device = device\n",
    "        self.bidirectional = bidirectional\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_first = batch_first\n",
    "        self.merge_bi = merge_bi\n",
    "        self.rnn_type = rnn_type.lower()\n",
    "\n",
    "        rnn_cls = nn.LSTM if self.rnn_type == 'lstm' else nn.GRU\n",
    "        self.rnn = rnn_cls(input_size,\n",
    "                           hidden_size,\n",
    "                           batch_first=batch_first,\n",
    "                           num_layers=layers,\n",
    "                           bidirectional=bidirectional)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.packed_sequence = packed_sequence\n",
    "        if packed_sequence:\n",
    "            self.pack = PackSequence(batch_first=batch_first)\n",
    "            self.unpack = PadPackedSequence(batch_first=batch_first)\n",
    "\n",
    "    def _merge_bi(self, forward, backward):\n",
    "        if self.merge_bi == 'sum':\n",
    "            return forward + backward\n",
    "        return torch.cat((forward, backward), dim=-1)\n",
    "\n",
    "    def _select_last_unpadded(self, out, lengths):\n",
    "        gather_dim = 1 if self.batch_first else 0\n",
    "        gather_idx = ((lengths - 1)  # -1 to convert to indices\n",
    "                      .unsqueeze(1)  # (B) -> (B, 1)\n",
    "                      .expand((-1, self.hidden_size))  # (B, 1) -> (B, H)\n",
    "                      # (B, 1, H) if batch_first else (1, B, H)\n",
    "                      .unsqueeze(gather_dim))\n",
    "        # Last forward for real length or seq (unpadded tokens)\n",
    "        last_out = out.gather(gather_dim, gather_idx).squeeze(gather_dim)\n",
    "        return last_out\n",
    "\n",
    "    def _final_output(self, out, lengths):\n",
    "        # Collect last hidden state\n",
    "        # Code adapted from https://stackoverflow.com/a/50950188\n",
    "        if not self.bidirectional:\n",
    "            return self._select_last_unpadded(out, lengths)\n",
    "\n",
    "        forward, backward = (out[..., :self.hidden_size],\n",
    "                             out[..., self.hidden_size:])\n",
    "        # Last backward corresponds to first token\n",
    "        last_backward_out = (backward[:, 0, :]\n",
    "                             if self.batch_first\n",
    "                             else backward[0, ...])\n",
    "        # Last forward for real length or seq (unpadded tokens)\n",
    "        last_forward_out = self._select_last_unpadded(forward, lengths)\n",
    "        return self._merge_bi(last_forward_out, last_backward_out)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        self.rnn.flatten_parameters()\n",
    "        if self.packed_sequence:\n",
    "            x, lengths = self.pack(x, lengths)\n",
    "        out, hidden = self.rnn(x)\n",
    "        if self.packed_sequence:\n",
    "            out = self.unpack(out, lengths)\n",
    "        out = self.drop(out)\n",
    "\n",
    "        last_timestep = self._final_output(out, lengths)\n",
    "        return out, last_timestep, hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1,16,kernel_size = [2,1], stride = [2,1],padding=0),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16,16,kernel_size = (2,1), stride = (2,1),padding=0),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(16,16,kernel_size = (10,1), stride = 1),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        \n",
    "        # /////////////////////////INCEPTION////////////////////\n",
    "        self.layer11 = nn.Sequential(\n",
    "            nn.Conv2d(16,32,kernel_size = (1,1), stride = 1,padding=0),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.layer12 = nn.Sequential(\n",
    "            \n",
    "            nn.Conv2d(32,32,kernel_size = (1,3), stride = 1,padding=0),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.layer21 = nn.Sequential(\n",
    "            \n",
    "            nn.Conv2d(16,32,kernel_size = (1,1), stride = 1,padding=0),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.layer22 = nn.Sequential(\n",
    "            nn.Conv2d(32,32,kernel_size = (1,5), stride = 1,padding=0),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.layer31 = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=(1,3),stride=1)\n",
    "        )\n",
    "        self.layer32 = nn.Sequential(\n",
    "            nn.Conv2d(16,32,kernel_size = (1,1), stride = 1,padding=0),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        #/////////////////////////LSTM/////////////////////\n",
    "        self.lstm = RNN(32,32,packed_sequence=False)\n",
    "#        nn.LSTM(32,30,1,batch_first=True)\n",
    "        #////////////////////////LINEAR///////////////////\n",
    "        self.linear = nn.Linear(32,3)\n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        x = out\n",
    "        out1 = self.layer11(x)\n",
    "        out1 = self.layer12(out1)\n",
    "        \n",
    "        out2 = self.layer21(x)\n",
    "        out2 = self.layer22(out2)\n",
    "        \n",
    "        out3 = self.layer31(x)\n",
    "        out3 = self.layer32(out3)\n",
    "        \n",
    "        out = torch.cat((out1,out2),-1)\n",
    "        out = torch.cat((out,out3),-1)\n",
    "        out = out.squeeze(2)\n",
    "        out = out.transpose(1,2)\n",
    "        \n",
    "        #print(out.shape)\n",
    "        #hidden_b = torch.zeros(1,out.shape[0],30).double()\n",
    "        #self.hiddens = (hidden_b, hidden_b) \n",
    "        _, lstm_out, self.hiddens = self.lstm.forward(out, torch.tensor([22,22,22,22,22,22,22,22,\n",
    "                                                                        22,22,22,22,22,22,22,22,\n",
    "                                                                        22,22,22,22,22,22,22,22,\n",
    "                                                                        22,22,22,22,22,22,22,22,\n",
    "                                                                        ]))\n",
    "        \n",
    "        #correctout = lstm_out.reshape(32,-1)\n",
    "#         #if correctout.shape[1] < 660:\n",
    "#             print('oops')\n",
    "#             needpading = 660-correctout.shape[1]\n",
    "#             padleft = int(needpading / 2)\n",
    "#             padright = int(needpading/2)\n",
    "#             if needpading != padleft+padright:\n",
    "#                 padright+= 1\n",
    "#             correctout = F.pad(correctout,(int(padleft),int(padright)))\n",
    "            \n",
    "        lastout = self.linear(lstm_out)\n",
    "        \n",
    "        return lastout\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "output_dim = 1\n",
    "epochs = 30\n",
    "BATCH_SZ = 32\n",
    "\n",
    "dl_train = DataLoader(train, batch_size = BATCH_SZ, shuffle=False)\n",
    "dl_test =  DataLoader(test, batch_size = BATCH_SZ, shuffle=False)\n",
    "\n",
    "cnn = Model().double()\n",
    "\n",
    "optimizer = optim.Adam(cnn.parameters(), lr = 0.01,eps=0.5)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0  out of  30\n",
      "[1832.10157688    0.            0.            0.            0.\n",
      "    0.            0.            0.            0.            0.\n",
      "    0.            0.            0.            0.            0.\n",
      "    0.            0.            0.            0.            0.\n",
      "    0.            0.            0.            0.            0.\n",
      "    0.            0.            0.            0.            0.        ]\n",
      "For Epoch: 0 train loss: 0.18841028145627386 val loss: 0.2340269831193431\n",
      "epoch:  1  out of  30\n",
      "[1.88410281e-01 1.27101357e+03 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00]\n",
      "For Epoch: 1 train loss: 0.13070892335803722 val loss: 0.2302650915126038\n",
      "epoch:  2  out of  30\n",
      "[1.88410281e-01 1.30708923e-01 1.24338915e+03 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00]\n",
      "For Epoch: 2 train loss: 0.12786807344452045 val loss: 0.2295474335630614\n",
      "epoch:  3  out of  30\n",
      "[1.88410281e-01 1.30708923e-01 1.27868073e-01 1.23038393e+03\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00]\n",
      "For Epoch: 3 train loss: 0.12653063817848054 val loss: 0.22953898520034913\n",
      "epoch:  4  out of  30\n",
      "[1.88410281e-01 1.30708923e-01 1.27868073e-01 1.26530638e-01\n",
      " 1.22040865e+03 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00]\n",
      "For Epoch: 4 train loss: 0.12550479733883718 val loss: 0.22970499867770816\n",
      "epoch:  5  out of  30\n",
      "[1.88410281e-01 1.30708923e-01 1.27868073e-01 1.26530638e-01\n",
      " 1.25504797e-01 1.21224183e+03 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00]\n",
      "For Epoch: 5 train loss: 0.12466493565914044 val loss: 0.22985200953883844\n",
      "epoch:  6  out of  30\n",
      "[1.88410281e-01 1.30708923e-01 1.27868073e-01 1.26530638e-01\n",
      " 1.25504797e-01 1.24664936e-01 1.20514125e+03 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00]\n",
      "For Epoch: 6 train loss: 0.1239347233857684 val loss: 0.2297009772764919\n",
      "epoch:  7  out of  30\n",
      "[1.88410281e-01 1.30708923e-01 1.27868073e-01 1.26530638e-01\n",
      " 1.25504797e-01 1.24664936e-01 1.23934723e-01 1.19896801e+03\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00]\n",
      "For Epoch: 7 train loss: 0.12329987772022 val loss: 0.22833565080553017\n",
      "epoch:  8  out of  30\n",
      "[1.88410281e-01 1.30708923e-01 1.27868073e-01 1.26530638e-01\n",
      " 1.25504797e-01 1.24664936e-01 1.23934723e-01 1.23299878e-01\n",
      " 1.19275787e+03 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00]\n",
      "For Epoch: 8 train loss: 0.12266123725818176 val loss: 0.22766602930725086\n",
      "epoch:  9  out of  30\n",
      "[1.88410281e-01 1.30708923e-01 1.27868073e-01 1.26530638e-01\n",
      " 1.25504797e-01 1.24664936e-01 1.23934723e-01 1.23299878e-01\n",
      " 1.22661237e-01 1.18641767e+03 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00]\n",
      "For Epoch: 9 train loss: 0.12200922161630924 val loss: 0.22688360310729386\n",
      "epoch:  10  out of  30\n",
      "[1.88410281e-01 1.30708923e-01 1.27868073e-01 1.26530638e-01\n",
      " 1.25504797e-01 1.24664936e-01 1.23934723e-01 1.23299878e-01\n",
      " 1.22661237e-01 1.22009222e-01 1.18076265e+03 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00]\n",
      "For Epoch: 10 train loss: 0.1214276687487568 val loss: 0.22606387107291662\n",
      "epoch:  11  out of  30\n",
      "[1.88410281e-01 1.30708923e-01 1.27868073e-01 1.26530638e-01\n",
      " 1.25504797e-01 1.24664936e-01 1.23934723e-01 1.23299878e-01\n",
      " 1.22661237e-01 1.22009222e-01 1.21427669e-01 1.17473530e+03\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00]\n",
      "For Epoch: 11 train loss: 0.1208078257909922 val loss: 0.22455594230211284\n",
      "epoch:  12  out of  30\n",
      "[1.88410281e-01 1.30708923e-01 1.27868073e-01 1.26530638e-01\n",
      " 1.25504797e-01 1.24664936e-01 1.23934723e-01 1.23299878e-01\n",
      " 1.22661237e-01 1.22009222e-01 1.21427669e-01 1.20807826e-01\n",
      " 1.16842242e+03 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00]\n",
      "For Epoch: 12 train loss: 0.12015861975960249 val loss: 0.22327171481615918\n",
      "epoch:  13  out of  30\n",
      "[1.88410281e-01 1.30708923e-01 1.27868073e-01 1.26530638e-01\n",
      " 1.25504797e-01 1.24664936e-01 1.23934723e-01 1.23299878e-01\n",
      " 1.22661237e-01 1.22009222e-01 1.21427669e-01 1.20807826e-01\n",
      " 1.20158620e-01 1.16291479e+03 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00]\n",
      "For Epoch: 13 train loss: 0.1195922245595942 val loss: 0.2216351052477573\n",
      "epoch:  14  out of  30\n",
      "[1.88410281e-01 1.30708923e-01 1.27868073e-01 1.26530638e-01\n",
      " 1.25504797e-01 1.24664936e-01 1.23934723e-01 1.23299878e-01\n",
      " 1.22661237e-01 1.22009222e-01 1.21427669e-01 1.20807826e-01\n",
      " 1.20158620e-01 1.19592225e-01 1.15821523e+03 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00]\n",
      "For Epoch: 14 train loss: 0.11910892944856243 val loss: 0.2208747175648834\n",
      "epoch:  15  out of  30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.88410281e-01 1.30708923e-01 1.27868073e-01 1.26530638e-01\n",
      " 1.25504797e-01 1.24664936e-01 1.23934723e-01 1.23299878e-01\n",
      " 1.22661237e-01 1.22009222e-01 1.21427669e-01 1.20807826e-01\n",
      " 1.20158620e-01 1.19592225e-01 1.19108929e-01 1.15413264e+03\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00]\n",
      "For Epoch: 15 train loss: 0.11868908219484249 val loss: 0.21957540009709367\n",
      "epoch:  16  out of  30\n",
      "[1.88410281e-01 1.30708923e-01 1.27868073e-01 1.26530638e-01\n",
      " 1.25504797e-01 1.24664936e-01 1.23934723e-01 1.23299878e-01\n",
      " 1.22661237e-01 1.22009222e-01 1.21427669e-01 1.20807826e-01\n",
      " 1.20158620e-01 1.19592225e-01 1.19108929e-01 1.18689082e-01\n",
      " 1.15055622e+03 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00]\n",
      "For Epoch: 16 train loss: 0.11832128983097824 val loss: 0.2183519830130649\n",
      "epoch:  17  out of  30\n",
      "[1.88410281e-01 1.30708923e-01 1.27868073e-01 1.26530638e-01\n",
      " 1.25504797e-01 1.24664936e-01 1.23934723e-01 1.23299878e-01\n",
      " 1.22661237e-01 1.22009222e-01 1.21427669e-01 1.20807826e-01\n",
      " 1.20158620e-01 1.19592225e-01 1.19108929e-01 1.18689082e-01\n",
      " 1.18321290e-01 1.14788812e+03 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00]\n",
      "For Epoch: 17 train loss: 0.11804690706406902 val loss: 0.21694937857578156\n",
      "epoch:  18  out of  30\n",
      "[1.88410281e-01 1.30708923e-01 1.27868073e-01 1.26530638e-01\n",
      " 1.25504797e-01 1.24664936e-01 1.23934723e-01 1.23299878e-01\n",
      " 1.22661237e-01 1.22009222e-01 1.21427669e-01 1.20807826e-01\n",
      " 1.20158620e-01 1.19592225e-01 1.19108929e-01 1.18689082e-01\n",
      " 1.18321290e-01 1.18046907e-01 1.14558865e+03 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00]\n",
      "For Epoch: 18 train loss: 0.11781043293301943 val loss: 0.2160837249410687\n",
      "epoch:  19  out of  30\n",
      "[1.88410281e-01 1.30708923e-01 1.27868073e-01 1.26530638e-01\n",
      " 1.25504797e-01 1.24664936e-01 1.23934723e-01 1.23299878e-01\n",
      " 1.22661237e-01 1.22009222e-01 1.21427669e-01 1.20807826e-01\n",
      " 1.20158620e-01 1.19592225e-01 1.19108929e-01 1.18689082e-01\n",
      " 1.18321290e-01 1.18046907e-01 1.17810433e-01 1.14384195e+03\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00]\n",
      "For Epoch: 19 train loss: 0.11763080567424526 val loss: 0.21523543823866817\n",
      "epoch:  20  out of  30\n",
      "[1.88410281e-01 1.30708923e-01 1.27868073e-01 1.26530638e-01\n",
      " 1.25504797e-01 1.24664936e-01 1.23934723e-01 1.23299878e-01\n",
      " 1.22661237e-01 1.22009222e-01 1.21427669e-01 1.20807826e-01\n",
      " 1.20158620e-01 1.19592225e-01 1.19108929e-01 1.18689082e-01\n",
      " 1.18321290e-01 1.18046907e-01 1.17810433e-01 1.17630806e-01\n",
      " 1.14223621e+03 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00]\n",
      "For Epoch: 20 train loss: 0.11746567333102491 val loss: 0.21460372522667492\n",
      "epoch:  21  out of  30\n",
      "[1.88410281e-01 1.30708923e-01 1.27868073e-01 1.26530638e-01\n",
      " 1.25504797e-01 1.24664936e-01 1.23934723e-01 1.23299878e-01\n",
      " 1.22661237e-01 1.22009222e-01 1.21427669e-01 1.20807826e-01\n",
      " 1.20158620e-01 1.19592225e-01 1.19108929e-01 1.18689082e-01\n",
      " 1.18321290e-01 1.18046907e-01 1.17810433e-01 1.17630806e-01\n",
      " 1.17465673e-01 1.14084939e+03 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00]\n",
      "For Epoch: 21 train loss: 0.1173230557155521 val loss: 0.2143574426497465\n",
      "epoch:  22  out of  30\n",
      "[1.88410281e-01 1.30708923e-01 1.27868073e-01 1.26530638e-01\n",
      " 1.25504797e-01 1.24664936e-01 1.23934723e-01 1.23299878e-01\n",
      " 1.22661237e-01 1.22009222e-01 1.21427669e-01 1.20807826e-01\n",
      " 1.20158620e-01 1.19592225e-01 1.19108929e-01 1.18689082e-01\n",
      " 1.18321290e-01 1.18046907e-01 1.17810433e-01 1.17630806e-01\n",
      " 1.17465673e-01 1.17323056e-01 1.13982441e+03 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00]\n",
      "For Epoch: 22 train loss: 0.11721764777843027 val loss: 0.21420863608802257\n",
      "epoch:  23  out of  30\n",
      "[1.88410281e-01 1.30708923e-01 1.27868073e-01 1.26530638e-01\n",
      " 1.25504797e-01 1.24664936e-01 1.23934723e-01 1.23299878e-01\n",
      " 1.22661237e-01 1.22009222e-01 1.21427669e-01 1.20807826e-01\n",
      " 1.20158620e-01 1.19592225e-01 1.19108929e-01 1.18689082e-01\n",
      " 1.18321290e-01 1.18046907e-01 1.17810433e-01 1.17630806e-01\n",
      " 1.17465673e-01 1.17323056e-01 1.17217648e-01 1.13876933e+03\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00]\n",
      "For Epoch: 23 train loss: 0.11710914526833738 val loss: 0.2144370366026097\n",
      "epoch:  24  out of  30\n",
      "[1.88410281e-01 1.30708923e-01 1.27868073e-01 1.26530638e-01\n",
      " 1.25504797e-01 1.24664936e-01 1.23934723e-01 1.23299878e-01\n",
      " 1.22661237e-01 1.22009222e-01 1.21427669e-01 1.20807826e-01\n",
      " 1.20158620e-01 1.19592225e-01 1.19108929e-01 1.18689082e-01\n",
      " 1.18321290e-01 1.18046907e-01 1.17810433e-01 1.17630806e-01\n",
      " 1.17465673e-01 1.17323056e-01 1.17217648e-01 1.17109145e-01\n",
      " 1.13757507e+03 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00]\n",
      "For Epoch: 24 train loss: 0.11698632926466644 val loss: 0.2151592461957632\n",
      "epoch:  25  out of  30\n",
      "[1.88410281e-01 1.30708923e-01 1.27868073e-01 1.26530638e-01\n",
      " 1.25504797e-01 1.24664936e-01 1.23934723e-01 1.23299878e-01\n",
      " 1.22661237e-01 1.22009222e-01 1.21427669e-01 1.20807826e-01\n",
      " 1.20158620e-01 1.19592225e-01 1.19108929e-01 1.18689082e-01\n",
      " 1.18321290e-01 1.18046907e-01 1.17810433e-01 1.17630806e-01\n",
      " 1.17465673e-01 1.17323056e-01 1.17217648e-01 1.17109145e-01\n",
      " 1.16986329e-01 1.13602120e+03 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00]\n",
      "For Epoch: 25 train loss: 0.11682653227405644 val loss: 0.21575213637480728\n",
      "epoch:  26  out of  30\n",
      "[1.88410281e-01 1.30708923e-01 1.27868073e-01 1.26530638e-01\n",
      " 1.25504797e-01 1.24664936e-01 1.23934723e-01 1.23299878e-01\n",
      " 1.22661237e-01 1.22009222e-01 1.21427669e-01 1.20807826e-01\n",
      " 1.20158620e-01 1.19592225e-01 1.19108929e-01 1.18689082e-01\n",
      " 1.18321290e-01 1.18046907e-01 1.17810433e-01 1.17630806e-01\n",
      " 1.17465673e-01 1.17323056e-01 1.17217648e-01 1.17109145e-01\n",
      " 1.16986329e-01 1.16826532e-01 1.13439017e+03 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00]\n",
      "For Epoch: 26 train loss: 0.11665880035339081 val loss: 0.21608013931994194\n",
      "epoch:  27  out of  30\n",
      "[1.88410281e-01 1.30708923e-01 1.27868073e-01 1.26530638e-01\n",
      " 1.25504797e-01 1.24664936e-01 1.23934723e-01 1.23299878e-01\n",
      " 1.22661237e-01 1.22009222e-01 1.21427669e-01 1.20807826e-01\n",
      " 1.20158620e-01 1.19592225e-01 1.19108929e-01 1.18689082e-01\n",
      " 1.18321290e-01 1.18046907e-01 1.17810433e-01 1.17630806e-01\n",
      " 1.17465673e-01 1.17323056e-01 1.17217648e-01 1.17109145e-01\n",
      " 1.16986329e-01 1.16826532e-01 1.16658800e-01 1.13285028e+03\n",
      " 0.00000000e+00 0.00000000e+00]\n",
      "For Epoch: 27 train loss: 0.11650044008741443 val loss: 0.21607644621370198\n",
      "epoch:  28  out of  30\n",
      "[1.88410281e-01 1.30708923e-01 1.27868073e-01 1.26530638e-01\n",
      " 1.25504797e-01 1.24664936e-01 1.23934723e-01 1.23299878e-01\n",
      " 1.22661237e-01 1.22009222e-01 1.21427669e-01 1.20807826e-01\n",
      " 1.20158620e-01 1.19592225e-01 1.19108929e-01 1.18689082e-01\n",
      " 1.18321290e-01 1.18046907e-01 1.17810433e-01 1.17630806e-01\n",
      " 1.17465673e-01 1.17323056e-01 1.17217648e-01 1.17109145e-01\n",
      " 1.16986329e-01 1.16826532e-01 1.16658800e-01 1.16500440e-01\n",
      " 1.13151428e+03 0.00000000e+00]\n",
      "For Epoch: 28 train loss: 0.11636304766181714 val loss: 0.2163853289466846\n",
      "epoch:  29  out of  30\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "zip argument #5 must support iteration",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-190-3952c489a86d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'epoch: '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m' out of '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdl_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontainer_abcs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: zip argument #5 must support iteration"
     ]
    }
   ],
   "source": [
    "\n",
    "min_loss = 10000000\n",
    "train_running_average_loss = np.zeros((epochs))\n",
    "val_running_average_loss = np.zeros((epochs))\n",
    "learning = np.zeros((epochs))\n",
    "for epoch in range(epochs):\n",
    "    print('epoch: ',epoch,' out of ',epochs)\n",
    "    \n",
    "    for i,data in enumerate (dl_train):\n",
    "        X_batch, y_batch = data\n",
    "        \n",
    "        X_batch.unsqueeze_(-1)\n",
    "        X_batch = X_batch.transpose(1,3)\n",
    "        #X_batch = X_batch.transpose(-1,1)\n",
    "        X_batch = X_batch.double()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        out = cnn(X_batch)\n",
    "    \n",
    "        loss = criterion(out, y_batch.long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_running_average_loss[epoch] += loss.detach().item()\n",
    "    print(train_running_average_loss)\n",
    "    train_running_average_loss[epoch] =  train_running_average_loss[epoch]/i\n",
    "        ###############################################    \n",
    "    # Now i ll run the validation set\n",
    "    for i, data in enumerate(dl_test):\n",
    "        X_batch, y_batch = data\n",
    "        \n",
    "        X_batch.unsqueeze_(-1)\n",
    "        X_batch = X_batch.transpose(1,3)\n",
    "        #X_batch = X_batch.transpose(-1,1)\n",
    "        X_batch = X_batch.double()\n",
    "        \n",
    "        out  = cnn(X_batch)\n",
    "        loss = criterion(out, y_batch.long())\n",
    "        \n",
    "        val_running_average_loss[epoch] += loss.detach().item()\n",
    "    val_running_average_loss[epoch] =  val_running_average_loss[epoch]/i\n",
    "    if val_running_average_loss[epoch] < min_loss:\n",
    "        min_loss = val_running_average_loss[epoch]\n",
    "        max_model = cnn\n",
    "    print('For Epoch:', epoch, 'train loss:', train_running_average_loss[epoch], 'val loss:', val_running_average_loss[epoch])\n",
    "    if epoch >1:\n",
    "        if 0.9*val_running_average_loss[epoch] > val_running_average_loss[epoch-1] :\n",
    "            break\n",
    "plt.figure()        \n",
    "plt.plot(train_running_average_loss, 'b', val_running_average_loss, 'g', epochs) \n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('train_loss blue, val_loss green')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (layer1): Sequential(\n",
      "    (0): Conv2d(1, 16, kernel_size=[2, 1], stride=[2, 1])\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Conv2d(16, 16, kernel_size=(2, 1), stride=(2, 1))\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Conv2d(16, 16, kernel_size=(10, 1), stride=(1, 1))\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "  )\n",
      "  (layer11): Sequential(\n",
      "    (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "  )\n",
      "  (layer12): Sequential(\n",
      "    (0): Conv2d(32, 32, kernel_size=(1, 3), stride=(1, 1))\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "  )\n",
      "  (layer21): Sequential(\n",
      "    (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "  )\n",
      "  (layer22): Sequential(\n",
      "    (0): Conv2d(32, 32, kernel_size=(1, 5), stride=(1, 1))\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "  )\n",
      "  (layer31): Sequential(\n",
      "    (0): MaxPool2d(kernel_size=(1, 3), stride=1, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer32): Sequential(\n",
      "    (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "  )\n",
      "  (lstm): RNN(\n",
      "    (rnn): LSTM(32, 32, batch_first=True)\n",
      "    (drop): Dropout(p=0, inplace=False)\n",
      "  )\n",
      "  (linear): Linear(in_features=32, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(max_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
